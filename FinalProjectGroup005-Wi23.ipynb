{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title: Fraudulent Transactions Detection",
    "\n",
    "## Group members\n",
    "\n",
    "- Xiaotong Zeng\n",
    "- Yandong Xiang\n",
    "- Xiang Li\n",
    "- Jiaxin Ye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables",
    "\n",
    "The goal of this project is to predict whether a transaction is fraudulent given identity and transaction information. The data that we are using contains two files: identity and transaction, which are joined by the “TransactionID” variable. But not all transactions have corresponding identifiers. The variables in the identity dataset are network connection information and digital signature associated with transactions, which are provided by Vesta’s fraud protection system and digital security partners. The variables in the transaction dataset are transaction amounts, card information, product of each transaction, and other related information. We will first apply data cleaning, and then data engineering to One-Hot-Encode categorical variables, standardize numerical variables, and perform the PCA for newly engineered variables. Then, we will use Naive Bayes as our baseline model, and we will compare it with the performances of RandomForest, LightGBM, and XGBoost to determine which model we will eventually use for this prediction task. In terms of evaluating the performance of our model, we will be using recall scores primarily since false negatives are more costly in fraud detection. We do not want to predict a transaction as normal when it is actually fraudulent. But we will also take precision, accuracy, and F1 score into consideration for the balance of our model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 2 or 3 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. ",
    "\n",
    "Nowadays, electronic payments have become an indispensable part of people’s everyday lives. It is an ongoing trend that digital wallets have replaced physical wallets. But as this trend evolves, many unscrupulous people see the opportunity of making illegal profits out of it, i.e. through transaction fraud. Online transaction fraud occurs when a fraudster attempts to conduct a transaction using another person’s identity and payment information<a name=\"Sasso\"></a>[<sup>[1]</sup>](#Sasso). According to the Federal Trade Commission (FTC), there was a total of 5.8 billion dollars of loss through fraud in 2021 for a 70 percent year-over-year increase<a name=\"Skiba\"></a>[<sup>[2]</sup>](#Skiba). In addition to ostensible monetary losses on the consumers’ sides, the fraud also induces negative impacts on customer/reputational, operational and topline revenue implications on the merchants’ sides<a name=\"McKee\"></a>[<sup>[3]</sup>](#McKee). If the merchants impose a restrictive fraud control policy, this can diminish the consumer experience. It is also time-consuming and labor-intensive for merchants to maintain anti-fraud technology. In terms of topline revenue, merchants may tend to avoid new revenue opportunities, such as entering a foreign market, due to concerns over fraud. Thus, it is of great importance to develop fraud detection algorithms to protect both consumers and merchants, and to provide a safe e-payment environment in general. In particular, machine learning techniques are widely used in fraud detection nowadays, and it is far more accurate than the traditional manual process. In the late 20th century, the companies spotted frauds through simple rules. For example, if a transaction exceeds 10,000 dollars, then there was a high risk of fraud. However, this was a static method since it could not keep up with the changing customer behavior promptly. That is, it was not effective in terms of adjusting to new fraud patterns. So, machine learning models are a better choice since it allows automation and self-adaptiveness. They learn which transactions are normal and which are anomalous<a name=\"Oakley\"></a>[<sup>[4]</sup>](#Oakley). However, a problem that we would encounter is the issue of imbalanced data. In the real world, most transactions are valid and only a small portion are fraud transactions<a name=\"Pykes\"></a>[<sup>[5]</sup>](#Pykes). Thus, we need to take this into account when we deal with our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).",
    "\n",
    "The problem is to predict whether a transaction is fraudulent using features such as card, address, email, device information, etc. Fraudulence is a big issue since it harms both customers and merchants, and the society in general. Historical methods of detecting fraud transactions were based on simple rules that may not be able to catch the complicated patterns in the data. The objective of this project is to predict if a transaction is fraudulent (label 1 means fraudulent, and label 0 means normal) through experimenting with different machine learning models and selecting the one with the highest recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n",
    "\n",
    "identity.csv\n",
    "- https://www.kaggle.com/competitions/ieee-fraud-detection/data?select=train_identity.csv\n",
    "- (41 variables, 144233 observations)\n",
    "- Consists of: DeviceType, DeviceInfo, id_01 - id_38, TransactionID\n",
    "    - DeviceType: type of device used, such as mobile and desktop, etc\n",
    "    - DeviceInfo: The system used by the device\n",
    "    - Transaction ID: id for the transaction made\n",
    "    - id_01 - id_38: network connection and digital signature information\n",
    "- Critical variables: id_12 - id_38(float), Device type(‘string’),  Device Info(‘string’)\n",
    "- Convert device type to is mobile: if is mobile, True, else False\n",
    "- Onehot encode Device Info, Their are 1786 unique device software. Keep the software in the top 20 ocurrances of the dataset and set rest of them as “Others” [to 21 columns].\n",
    "\n",
    "\n",
    "transaction.csv\n",
    "- https://www.kaggle.com/competitions/ieee-fraud-detection/data?select=train_transaction.csv\n",
    "- (394 variables, 590540 observations)\n",
    "- Consists of: ProductCD, card1 - card6, addr1, addr2, P_emaildomain, R_emaildomain, M1-M9, C1-C14, D1-D15, V1 - V339, TransactionDT, TransactionAmt, dist1 - dist2\n",
    "    - isFraud: the label of dataset\n",
    "    - ProductCD: The code of the Product \n",
    "    - TransactionAmt: Transaction amount\n",
    "    - TransactionDT: Transaction datetime\n",
    "    - P_emaildomain: purchaser email domain\n",
    "    - R_emaildomain: recipient email domain\n",
    "    - card1 - card6: payment card information\n",
    "    - addr1: billing region\n",
    "    - addr2: billing country\n",
    "    - dist1 - dist2: distance between different address\n",
    "    - M1-M9: matches between info provided\n",
    "    - C1-C14: count of address associated with the card\n",
    "    - D1-D15: days between previous transactions\n",
    "    - V1 - V339: Vesta engineered rich features\n",
    "- Critical variables: card1 - card6(Int), addr1(float), addr2(float), M1-M9(boolean), C1-C14(float), D1-D15(float), V1 - V339(float), TransactionDT, TransactionAmt, dist1 - dist2, ProductCD(string), P_emaildomain(string), R_emaildomain(string), TransactionAmt(float)\n",
    "- Convert ProductCD to is W: if is W, True, else False.\n",
    "- Onehot encode P_emaildomain[to 59 columns] and R_emaildomain[to 60 columns].\n",
    "\n",
    "\n",
    "Data Merging:\n",
    "- Merging on the TransactionID column of the identity.csv and the TransactionID column of the transaction.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. ",
    "\n",
    "To predict whether a transaction is fraudulent or not, we will train and compare several supervised machine learning classifiers using the information about the transactions and the user identities. The first step is to join the transaction table and identity table by transaction id. We will clean up the dataset by removing or imputing the missing or invalid data. Next, we will perform feature engineering by implementing one-hot encoding for the categorical features, standardization for the numerical features, and principal component analysis for all the newly engineered features. Afterwards, we will select the top features and train different classifiers on the feature data. For model selections, we will try Naive Bayes, RandomForest, LightGBM, and XGBoost as they are more suitable for classification tasks and can be trained on large datasets with high-dimensional feature spaces. For performance evaluation, we will split the data into training and testing sets, and use metrics such as accuracy, precision, recall, and F1 score to measure the performance of the classifier on the testing set. We will also use cross-validation to prevent overfitting and use grid search to tune the hyperparameters of the classifier. \n",
    "In terms of implementation, we will use scikit-learn, lightGBM, and XGBoost libraries/packages and their functions to preprocess the data and construct the models. For our benchmark model, we plan to use a Naive Bayes classifier and compare the performance of the other models (RandomForest, LightGBM, and XGBoost) against it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).",
    "\n",
    "For each observation in the test set, we will predict whether or not the transaction is fraud. To evaluate our model performance, we will be mainly using Confusion Matrix to assess our predictions. In the case of fraud detection, the cost of False Negatives is much higher than the cost of False Positives. To put it into context, we would rather predict transactions that are not fraudulent as fraudulent than predict some transactions that are fraudulent as not fraudulent. Therefore, we need to focus on getting higher recall. Meanwhile, we will also control for the balance of our model predictions by looking at accuracy, precision, and F1 score.\n",
    "\n",
    "- Recall = True Positive / (True Positive + False Negative)\n",
    "- Precision = True Positive / (True Positive + False Positive)\n",
    "- F1 = (2 * precision * recall) / (precision + recall)\n",
    "- Accuracy = (True Positive + True Negative) / (True Positive + False Negative + True Negative + False Positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "First of all, since this is a public dataset found on Kaggle, it’s likely that it is rather safe regarding ethics and data privacy concerns. Upon examining the dataset, although we have transaction id, there’s no way we could trace it back to find personally identifiable information. Moreover, although we have payment card information, we don’t have card number, but only card type, card category, issue bank, and country, which also isn’t enough to extract personally identifiable information. For the identity part of this dataset, it’s unlikely that there could be ethical or privacy concerns because we don’t actually know what the fields stand for, as they are simply called id2, id3, etc. According to the descriptions, The field names are masked and pairwise dictionaries will not be provided for privacy protection and contract agreement. However, this dataset could be somewhat biased because according to its description, the data is collected by Vesta’s fraud protection system and digital security partners, which is only one or several companies. Thus, this dataset is not guaranteed to be an accurate representation of all transactions that are fraud or not. It is also unlikely that our results could be used unethically because our task tries to identify and prevent frauds, without violating data privacy and without proposing individuals to engage in these illegal activities. ",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Sasso\"></a>1.[^](#Sasso): Sasso, T. (14 Feb 2023) What is Online Transaction Fraud? *Funraise.* https://help.funraise.io/en/articles/3944864-what-is-online-transaction-fraud#:~:text=What%20is%20Transaction%20Fraud%3F%20Transaction%20fraud%20is%20a,a%20transaction%20in%20their%20name%E2%80%94like%20stolen%20credit%20cards. <br> \n",
    "<a name=\"Skiba\"></a>2.[^](#Skiba): Skiba, K. (22 Feb 2022) Consumer Fraud Losses Hit Record $5.8 Billion. *AARP.* https://www.aarp.org/money/scams-fraud/info-2022/ftc-fraud-report-new.html <br>\n",
    "<a name=\"McKee\"></a>3.[^](#McKee): McKee, J. (22 Nov 2022) Unpacking The Overall Impact of Fraud. *Forbes.* https://www.forbes.com/sites/jordanmckee/2020/11/22/unpacking-the-overall-impact-of-fraud/?sh=560a75917891 <br>\n",
    "<a name=\"Oakley\"></a>4.[^](#Oakley): Oakley, C. (30 March 2022) The role of machine learning in fraud detection. *Featurespace.* https://www.featurespace.com/newsroom/the-role-of-machine-learning-in-fraud-detection/<br> \n",
    "<a name=\"Pykes\"></a>5.[^](#Pykes): Pykes, K. (21 Sep 2020) Using Machine Learning To Detect Fraud. *Towards Data Science.* https://towardsdatascience.com/using-machine-learning-to-detect-fraud-f204910389cf <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
