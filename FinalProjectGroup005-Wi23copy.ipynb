{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title: Fraudulent Transactions Detection\n",
    "## Group members\n",
    "\n",
    "- Xiaotong Zeng\n",
    "- Yandong Xiang\n",
    "- Xiang Li\n",
    "- Jiaxin Ye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "The goal of this project is to predict whether a transaction is fraudulent given identity and transaction information. The dataset that we are using contains two files: identity and transaction, which are joined by the “TransactionID” variable. The identity dataset contains network connection information and digital signature associated with transactions, and the transaction dataset contains transaction amounts, card information, product of each transaction, and other related information. We will first perform data cleaning, data engineering, and PCA on the datasets. Then, we will use Naive Bayes as our baseline model, and we will compare it with the performances of RandomForest, LightGBM, and XGBoost to determine which model we will eventually use for this prediction task. In terms of evaluating the performance of our model, we will be using recall scores primarily since false negatives are more costly in fraud detection, so we do not want to predict a transaction as normal when it is actually fraudulent. But we will also take precision, accuracy, and F1 score into consideration for the balance of our model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Nowadays, electronic payments have become an indispensable part of people’s everyday lives. It is an ongoing trend that digital wallets have replaced physical wallets. But as this trend evolves, many unscrupulous people see the opportunity of making illegal profits out of it, i.e. through transaction fraud. Online transaction fraud occurs when a fraudster attempts to conduct a transaction using another person’s identity and payment information<a name=\"Sasso\"></a>[<sup>[1]</sup>](#Sasso). According to the Federal Trade Commission (FTC), there was a total of 5.8 billion dollars of loss through fraud in 2021 for a 70 percent year-over-year increase<a name=\"Skiba\"></a>[<sup>[2]</sup>](#Skiba).<br>\n",
    "<br>\n",
    "In addition to ostensible monetary losses on the consumers’ sides, the fraud also induces negative impacts on customer/reputational, operational and topline revenue implications on the merchants’ sides<a name=\"McKee\"></a>[<sup>[3]</sup>](#McKee). If the merchants impose a restrictive fraud control policy, this can diminish the consumer experience. It is also time-consuming and labor-intensive for merchants to maintain anti-fraud technology. In terms of topline revenue, merchants may tend to avoid new revenue opportunities, such as entering a foreign market, due to concerns over fraud. Thus, it is of great importance to develop fraud detection algorithms to protect both consumers and merchants, and to provide a safe e-payment environment in general.<br>\n",
    "<br>\n",
    "In particular, machine learning techniques are widely used in fraud detection nowadays, and it is far more accurate than the traditional manual process. In the late 20th century, the companies spotted frauds through simple rules. For example, if a transaction exceeds 10,000 dollars, then there was a high risk of fraud. However, this was a static method since it could not keep up with the changing customer behavior promptly. That is, it was not effective in terms of adjusting to new fraud patterns. So, machine learning models are a better choice since it allows automation and self-adaptiveness. They learn which transactions are normal and which are anomalous<a name=\"Oakley\"></a>[<sup>[4]</sup>](#Oakley).<br>\n",
    "<br>\n",
    "However, a problem that we would encounter is the issue of imbalanced data. In the real world, most transactions are valid and only a small portion are fraud transactions<a name=\"Pykes\"></a>[<sup>[5]</sup>](#Pykes). Thus, we need to take this into account when we deal with our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem is to predict whether a transaction is fraudulent using features such as card, address, email, device information, etc. Fraudulence is a big issue since it harms both customers and merchants, and the society in general. Historical methods of detecting fraud transactions were based on simple rules that may not be able to catch the complicated patterns in the data. The objective of this project is to predict if a transaction is fraudulent (label 1 means fraudulent, and label 0 means normal) through experimenting with different machine learning models and selecting the one with the highest recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "### Meta Data\n",
    "\n",
    "identity.csv\n",
    "- https://www.kaggle.com/competitions/ieee-fraud-detection/data?select=train_identity.csv\n",
    "- (41 variables, 144233 observations)\n",
    "- Consists of: DeviceType, DeviceInfo, id_01 - id_38, TransactionID\n",
    "    - DeviceType: type of device used, such as mobile and desktop, etc\n",
    "    - DeviceInfo: The system used by the device\n",
    "    - Transaction ID: id for the transaction made\n",
    "    - id_01 - id_38: network connection and digital signature information\n",
    "- Critical variables: id_12 - id_38(float), Device type(‘string’),  Device Info(‘string’)\n",
    "- Convert device type to is mobile: if is mobile, True, else False\n",
    "- Onehot encode Device Info, Their are 1786 unique device software. Keep the software in the top 20 ocurrances of the dataset and set rest of them as “Others” [to 21 columns].\n",
    "\n",
    "\n",
    "transaction.csv\n",
    "- https://www.kaggle.com/competitions/ieee-fraud-detection/data?select=train_transaction.csv\n",
    "- (394 variables, 590540 observations)\n",
    "- Consists of: ProductCD, card1 - card6, addr1, addr2, P_emaildomain, R_emaildomain, M1-M9, C1-C14, D1-D15, V1 - V339, TransactionDT, TransactionAmt, dist1 - dist2\n",
    "    - isFraud: the label of dataset\n",
    "    - ProductCD: The code of the Product \n",
    "    - TransactionAmt: Transaction amount\n",
    "    - TransactionDT: Transaction datetime\n",
    "    - P_emaildomain: purchaser email domain\n",
    "    - R_emaildomain: recipient email domain\n",
    "    - card1 - card6: payment card information\n",
    "    - addr1: billing region\n",
    "    - addr2: billing country\n",
    "    - dist1 - dist2: distance between different address\n",
    "    - M1-M9: matches between info provided\n",
    "    - C1-C14: count of address associated with the card\n",
    "    - D1-D15: days between previous transactions\n",
    "    - V1 - V339: Vesta engineered rich features\n",
    "- Critical variables: card1 - card6(Int), addr1(float), addr2(float), M1-M9(boolean), C1-C14(float), D1-D15(float), V1 - V339(float), TransactionDT, TransactionAmt, dist1 - dist2, ProductCD(string), P_emaildomain(string), R_emaildomain(string), TransactionAmt(float)\n",
    "- Convert ProductCD to is W: if is W, True, else False.\n",
    "- Onehot encode P_emaildomain[to 59 columns] and R_emaildomain[to 60 columns].\n",
    "\n",
    "### Data Preprocessing\n",
    "Due to the abundance of transaction data as compared to identity data, we have chosen to utilize transaction data as the primary predictors in our analysis. To achieve this, we have performed a left join on the \"TransactionID\" column, merging the two dataframes. This merge retains all the records from the transaction dataframe and adds matched records from the identity dataframe. Consequently, our final dataset is a merged dataset containing 590540 rows and 434 columns. Because we have a total of 434 columns, it is difficult to check correlation between each column and the predicted label. Therefore, we used our intuition and domain knowledge to select a subset of features that we thought could be effective in predicting whether or not the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_path = \"data/train_transaction.csv\"\n",
    "train_identity_path = \"data/train_identity.csv\"\n",
    "train_transaction = pd.read_csv(train_transaction_path)\n",
    "train_identity = pd.read_csv(train_identity_path)\n",
    "train_df = pd.merge(train_identity, train_transaction, on=\"TransactionID\", how='right')\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the issue of unbalanced labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed significantly less fraud data in our dataset. In order to create a model that predicts both class accurately, we decide to balance the labels by downsampling, as oversampling would lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_is_fraud = train_df[train_df['isFraud']==1].sample(replace=False, n=len(train_df[train_df['isFraud']==1]))\n",
    "train_is_not_fraud = train_df[train_df['isFraud']==0].sample(replace=False, n=len(train_is_fraud))\n",
    "train_df = pd.concat([train_is_fraud, train_is_not_fraud])\n",
    "print(train_df.shape)\n",
    "train_df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "1. **Convert TransactionDT into Transaction Day and Transaction Hour**\n",
    "    - Justification: TransactionDT measures the time from a given reference datetime (not an actual timestamp), therefore, in order to better utilize the cycling attribute of time. We decide to convert TransactionDT to specific day and hours. In our EDAs, we also tried to convert TransactionDT into week and month. However, we did not have any particular patterns that differentiates the weeks or months. Therefore, we will only keep day and hour in our final table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform TransactionDT into Hour, Day of Week\n",
    "def convert_hour(x):\n",
    "    return (x // (1 * 60 * 60)) % 24\n",
    "\n",
    "def convert_day_of_week(x):\n",
    "    return (x // (24 * 60 * 60)) % 7\n",
    "\n",
    "train_df['TransactionDT'] = train_df['TransactionDT'] - min(train_df['TransactionDT'])\n",
    "train_df['TransactionHour'] = train_df['TransactionDT'].apply(convert_hour)\n",
    "train_df['TransactionDayOfWeek'] = train_df['TransactionDT'].apply(convert_day_of_week)\n",
    "\n",
    "fraud_hour_perc = pd.DataFrame(train_df.groupby(['isFraud', 'TransactionHour'])['isFraud'].count()).rename(columns={\"isFraud\": \"Fraud_Perc\"})\n",
    "fraud_hour_perc_df = fraud_hour_perc / fraud_hour_perc.groupby(level=0).sum()\n",
    "fraud_hour_perc_df = fraud_hour_perc_df.reset_index()\n",
    "fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "ax1.set_title('Fraud Percentage By Hour', fontsize=14)\n",
    "ax1 = sns.barplot(x=\"TransactionHour\", y=\"Fraud_Perc\", hue=\"isFraud\", palette=\"pastel\", data=fraud_hour_perc_df)\n",
    "ax1.tick_params(axis='y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2 = sns.lineplot(x='TransactionHour', y='Fraud_Perc', data = fraud_hour_perc_df, sort=False, hue=\"isFraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_dow_perc = pd.DataFrame(train_df.groupby(['isFraud', 'TransactionDayOfWeek'])['isFraud'].count()).rename(columns={\"isFraud\": \"Fraud_Perc\"})\n",
    "fraud_dow_perc_df = fraud_dow_perc / fraud_dow_perc.groupby(level=0).sum()\n",
    "fraud_dow_perc_df = fraud_dow_perc_df.reset_index()\n",
    "fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "ax1.set_title('Fraud Percentage By Day of Week', fontsize=14)\n",
    "ax1 = sns.barplot(x=\"TransactionDayOfWeek\", y=\"Fraud_Perc\", hue=\"isFraud\", palette=\"pastel\", data=fraud_dow_perc_df)\n",
    "ax1.tick_params(axis='y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2 = sns.lineplot(x='TransactionDayOfWeek', y='Fraud_Perc', data = fraud_dow_perc_df, sort=False, hue=\"isFraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Apply Log Transformation to Transaction Amount**\n",
    "    - In our EDA, we used a boxplot to plot the distribution of the transaction amount and found that the transaction amount is significantly skewed to the right which means that most of our transactions are below $200. In order to reduce the skewness of the data, we decide to apply a log transformation to the Transaction Amount to make the variable seem more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "transaction_amount = train_df[train_df['TransactionAmt'] < 1000]['TransactionAmt']\n",
    "ax = sns.boxplot(transaction_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bin_amt(x):\n",
    "    if x < 50:\n",
    "        return '$0-50'\n",
    "    elif x < 100:\n",
    "        return '$50-100'\n",
    "    elif x < 200:\n",
    "        return '$100-200'\n",
    "    elif x < 300:\n",
    "        return '$200-300'\n",
    "    elif x < 400:\n",
    "        return '$300-400'\n",
    "    elif x < 500:\n",
    "        return '$400-500'\n",
    "    elif x < 600:\n",
    "        return '$500-600'\n",
    "    elif x < 700:\n",
    "        return '$600-700'\n",
    "    elif x < 800:\n",
    "        return '$700-800'\n",
    "    elif x < 900:\n",
    "        return '$800-900'\n",
    "    elif x < 1000:\n",
    "        return '$900-1000'\n",
    "    elif x < 1500:\n",
    "        return '$1000-1500'\n",
    "    elif x < 2000:\n",
    "        return '$1500-2000'\n",
    "    elif x < 2500:\n",
    "        return '$2000-2500'\n",
    "    elif x < 3000:\n",
    "        return '$2500-3000'\n",
    "    elif x < 3500:\n",
    "        return '$3000-3500'\n",
    "    elif x < 4000:\n",
    "        return '$3500-4000'\n",
    "    elif x < 4500:\n",
    "        return '$4000-4500'\n",
    "    elif x < 5000:\n",
    "        return '$4500-5000'\n",
    "    elif x < 5500:\n",
    "        return '$5000-5500'\n",
    "    elif x < 6000:\n",
    "        return '$5500-6000'\n",
    "    elif x < 6500:\n",
    "        return '$6000-6500'\n",
    "    elif x < 7000:\n",
    "        return '$6500-7000'\n",
    "    elif x < 7500:\n",
    "        return '$7000-7500'\n",
    "    elif x < 8000:\n",
    "        return '$7500-8000'\n",
    "    elif x < 8500:\n",
    "        return '$8000-8500'\n",
    "    elif x < 9000:\n",
    "        return '$8500-9000'\n",
    "    elif x < 9500:\n",
    "        return '$9000-9500'\n",
    "    elif x < 10000:\n",
    "        return '$9500-10000'\n",
    "    else:\n",
    "        return '$10000-+'\n",
    "    \n",
    "train_df['TransactionAmt_bin2'] = train_df['TransactionAmt'].apply(convert_to_bin_amt)\n",
    "fraud_amount = pd.DataFrame(train_df.groupby(['isFraud', 'TransactionAmt_bin2'])['isFraud'].count()).rename(columns={\"isFraud\": \"Fraud_Perc\"})\n",
    "fraud_amount_df = fraud_amount / fraud_amount.groupby(level=0).sum()\n",
    "fraud_amount_df = fraud_amount_df.reset_index()\n",
    "fraud_amount_df['index_2'] = fraud_amount_df['TransactionAmt_bin2'].apply(lambda x: float(x.split('-')[0][1:]))\n",
    "fraud_amount_df = fraud_amount_df.sort_values(by=['index_2'], ascending=True)\n",
    "fig, ax1 = plt.subplots(figsize=(25,6))\n",
    "ax1.set_title('Fraud Percentage By Transaction Amount', fontsize=14)\n",
    "ax1 = sns.barplot(x=\"TransactionAmt_bin2\", y=\"Fraud_Perc\", hue=\"isFraud\", palette=\"pastel\", data=fraud_amount_df)\n",
    "ax1.tick_params(axis='y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2 = sns.lineplot(x='TransactionAmt_bin2', y='Fraud_Perc', data=fraud_amount_df, sort=True, hue=\"isFraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **One-hot encode the Addresses, Email Domains, and Device Infos, IDs Infos into popular values and others**\n",
    "    - During our exploratory analysis, we realized that certain addresses, email domains, and device infos are more likely to be associated with fraudulent transactions. Originally, we plan to one hot encode all the values in these columns. However, there are too many unique values in these columns and some of them do not have an siginificant influence on predicting whether or not the transaction is fraudulent. Therefore, in order to simplify our one-hot features but also capturing all the relevant values, we decide to keep the labels for the most common values (top 100 values) and transform the rest of the less frequent values into another category called \"Others\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addr1_transform(x):\n",
    "    if x in top_addr1:\n",
    "        return x\n",
    "    elif (pd.isnull(x) == True):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def addr2_transform(x):\n",
    "    if x in top_addr2:\n",
    "        return x\n",
    "    elif (pd.isnull(x) == True):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def email_p_transform(x):\n",
    "    if x in top_email_p:\n",
    "        return x\n",
    "    elif (pd.isnull(x) == True):\n",
    "        return 'Missing'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "def email_r_transform(x):\n",
    "    if x in top_email_r:\n",
    "        return x\n",
    "    elif (pd.isnull(x) == True):\n",
    "        return 'Missing'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "def device_info_transform(x):\n",
    "    if x in top_device_info:\n",
    "        return x\n",
    "    elif (pd.isnull(x) == True):\n",
    "        return 'Missing'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "def transform_i(x):\n",
    "    if x in top_i:\n",
    "        return str(x)\n",
    "    elif (pd.isnull(x) == True):\n",
    "        return 'Missing'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# Convert transactionDT into day of week and hour\n",
    "train_df['TransactionDT'] = train_df['TransactionDT'] - min(train_df['TransactionDT'])\n",
    "train_df['TransactionDayOfWeek'] = train_df['TransactionDT'].apply(convert_day_of_week)\n",
    "train_df['TransactionHour'] = train_df['TransactionDT'].apply(convert_hour)\n",
    "\n",
    "# Transaction Amount (Log)\n",
    "train_df['TransactionAmt_log'] = train_df['TransactionAmt'].apply(lambda x: np.log(x))\n",
    "\n",
    "#addr1, addr2\n",
    "top_addr1 = list(train_df['addr1'].value_counts().index)[:100] #top ten regions\n",
    "top_addr2 = list(train_df['addr2'].value_counts().index)[:100] #top ten countries\n",
    "train_df['addr1_new'] = train_df['addr1'].apply(addr1_transform)\n",
    "train_df['addr2_new'] = train_df['addr2'].apply(addr2_transform)\n",
    "\n",
    "#Email_Domain\n",
    "top_email_p = list(train_df['P_emaildomain'].value_counts().index)[:100] \n",
    "top_email_r = list(train_df['R_emaildomain'].value_counts().index)[:100] \n",
    "train_df['P_emaildomain'].value_counts()\n",
    "train_df['P_emaildomain_new'] = train_df['P_emaildomain'].apply(email_p_transform)\n",
    "train_df['R_emaildomain_new'] = train_df['R_emaildomain'].apply(email_r_transform)\n",
    "\n",
    "#Device\n",
    "top_device_info = list(train_df['DeviceInfo'].value_counts().index)[:100] #top device info\n",
    "train_df['DeviceInfo_new'] = train_df['DeviceInfo'].apply(device_info_transform)\n",
    "\n",
    "#IDs\n",
    "id_lst = ['id_01', 'id_03', 'id_04', 'id_05', 'id_06', 'id_09', 'id_10', 'id_11', 'id_13', 'id_14', \n",
    "               'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_33']\n",
    "new_id_lst = [i + '_new' for i in id_lst]\n",
    "top_i_lst = []\n",
    "for i in id_lst:\n",
    "    new_col = i + '_new'\n",
    "    top_i = list(train_df[i].value_counts().index)[:100]\n",
    "    top_i_lst.append(top_i)\n",
    "    train_df[new_col] = train_df[i].apply(transform_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Replace numerical missing values with 0 and replace any invalid number with -10 and 10**\n",
    "    - Since there are many columns with large number of missing data, we decide to replace the missing data for numerical columns with 0 as it will most likely not affect our prediction. Some invalid numbers such as infinity or negative infinity would cause issues in our predictions. Therefore, we decide to replace these invalid number with large numbers such as -10 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To predict whether a transaction is fraudulent or not, we will train and compare several supervised machine learning classifiers using the information about the transactions and the user identities. The first step is to join the transaction table and identity table by transaction id. We will clean up the dataset by removing or imputing the missing or invalid data. Next, we will perform feature engineering by implementing one-hot encoding for the categorical features, standardization for the numerical features, and principal component analysis for all the newly engineered features. Afterwards, we will select the top features and train different classifiers on the feature data. For model selections, we will try Naive Bayes, RandomForest, LightGBM, and XGBoost as they are more suitable for classification tasks and can be trained on large datasets with high-dimensional feature spaces. For performance evaluation, we will split the data into training and testing sets, and use metrics such as accuracy, precision, recall, and F1 score to measure the performance of the classifier on the testing set. We will also use cross-validation to prevent overfitting and use grid search to tune the hyperparameters of the classifier. \n",
    "In terms of implementation, we will use scikit-learn, lightGBM, and XGBoost libraries/packages and their functions to preprocess the data and construct the models. For our benchmark model, we plan to use a Naive Bayes classifier and compare it with the performance of the other models (RandomForest, LightGBM, and XGBoost). We will also use grid search to find the optimal hyperparameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For each observation in the test set, we will predict whether or not the transaction is fraud. To evaluate our model performance, we will be mainly using Confusion Matrix to assess our predictions. In the case of fraud detection, the cost of False Negatives is much higher than the cost of False Positives. To put it into context, we would rather predict transactions that are not fraudulent as fraudulent than predict some transactions that are fraudulent as not fraudulent. **Therefore, we need to focus on getting higher recall.** Meanwhile, we will also control for the balance of our model predictions by looking at accuracy, precision, and F1 score.\n",
    "\n",
    "- Recall = True Positive / (True Positive + False Negative)\n",
    "- Precision = True Positive / (True Positive + False Positive)\n",
    "- F1 = (2 * precision * recall) / (precision + recall)\n",
    "- Accuracy = (True Positive + True Negative) / (True Positive + False Negative + True Negative + False Positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "\n",
    "### 1: Motivation behind Algorithm Selections\n",
    "\n",
    "Because fraud detection is a binary classification task, we narrowed our selection of algorithms down to Bernoulli Naive Bayes, Random Forest as some basic classifiers. To reach for higher efficiency and accuracy, we also experimented with LightGBM and XGBoost classifiers. \n",
    "\n",
    "**Naive Bayes**\n",
    "- For Naive Bayes, it is a popular probabilistic classifier that works by calculating the probability of a data point belonging to each class based on the input features, and then choosing the class with the highest probability. In fraud detection, Naive Bayes can be used to calculate the probability of a transaction being fraudulent based on input features such as transaction amount, location, and time of day. One of the advantages of Naive Bayes is that it is computationally efficient and can handle a large number of input features. However, it makes the assumption that all input features are independent of each other, which may not always be the case in real-world datasets. \n",
    "\n",
    "**Random Forest**\n",
    "- Random forest is an ensemble method that combines multiple decision trees to improve performance and reduce the chance of overfitting. Random forest can be an effective classifier for fraud detection because it can handle a large number of input features and can identify the most important features for classification.\n",
    "\n",
    "**LightGBM**\n",
    "- LightGBM also works well when we have a large dataset with many features. It is a gradient boosting framework that uses decision trees as the base learners and can handle binary classification tasks well. One of the advantages of LightGBM is the speed and scalability. It can train models much faster than other gradient boosting frameworks like XGBoost. Additionally, LightGBM can handle missing data and can automatically handle categorical features.\n",
    "\n",
    "**XGBoost**\n",
    "- XGBoost can also be used for fraud detection because it is really effective for handling imbalanced dataset. The nature of fraud detection means that we will have more authentic transactions than fraud transactions. Therefore, handling imbalanced data could be critical in helping us detect the fraud transactions. XGBoost is also a gradient boosting framework that uses decision trees as the base learners and it is really effective in handling missing data and categorical features.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering - Flame\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for one-hot\n",
    "one_hot_lst = ['DeviceType', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_32', 'id_34', \n",
    "               'id_35', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'TransactionDayOfWeek', 'TransactionHour',\n",
    "                'ProductCD', 'card4', 'card6', 'addr1_new', 'addr2_new', 'P_emaildomain_new', 'R_emaildomain_new',\n",
    "               'DeviceInfo_new', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9'] + new_id_lst\n",
    "v_col = ['V' + str(i) for i in range(1, 340)]\n",
    "num_list = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n",
    "        'dist1', 'dist2', 'TransactionAmt_log'] + v_col\n",
    "\n",
    "to_ohe = train_df[one_hot_lst]\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(to_ohe)\n",
    "ohe_features = ohe.transform(to_ohe)\n",
    "cates = pd.DataFrame(ohe_features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA on numerical variables\n",
    "pca = PCA(n_components=4)\n",
    "nums_part = train_df[num_list].reset_index(drop=True)\n",
    "nums_part = nums_part.fillna(0).replace(-np.Inf, -10).replace(np.Inf, 10)\n",
    "pca.fit(nums_part)\n",
    "# Plot the cumulative explained variance as a function of the number of components\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "nums_part_pca = pca.transform(nums_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA on categorical variables\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(cates)\n",
    "# Plot the cumulative explained variance as a function of the number of components\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "cates_pca = pca.transform(cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([pd.DataFrame(nums_part_pca), pd.DataFrame(cates_pca)], axis=1)\n",
    "y = train_df['isFraud']\n",
    "X.columns = [x for x in range(len(X.columns.tolist()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Algorithms Comparison through K-Folds CV\n",
    "\n",
    "To get a more consistant result, we used K-Fold validations to access our model performance. We first randomly partitioned our data into ten equal parts. One of them is selected to be the test set and the remaining nine parts are used for training. We will train the model on the training set and evaluate its performance on the test set. We will repeat the process ten times and record the average of the accuracy, precision, recall, and f1-score. The purpose of this technique is to provide an estimate of how well the model is likely to perform on unseen data. By testing the model on multiple partitions of the data, the 10-fold cross-validation method provides a more robust estimate of the model's performance than a simple train test split. It can also help to avoid issues such as overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv(k, X, y, model):\n",
    "    kf = KFold(n_splits=k)\n",
    "    fold = 0\n",
    "    clf = model\n",
    "    acc_lst, pre_lst, rec_lst, f1_score_lst= [],[],[],[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #Split data        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        score = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "        acc_lst.append(accuracy)\n",
    "        pre_lst.append(score[0])\n",
    "        rec_lst.append(score[1])\n",
    "        f1_score_lst.append(score[2])\n",
    "        fold+=1\n",
    "        \n",
    "    accuracy = np.mean(acc_lst)\n",
    "    precision = np.mean(pre_lst)\n",
    "    recall = np.mean(rec_lst)\n",
    "    f1 = np.mean(f1_score_lst)\n",
    "    print('Accuracy = ' + str(round(accuracy, 3)))\n",
    "    print('Precision = ' + str(round(precision, 3)))\n",
    "    print('Recall = ' + str(round(recall, 3)))\n",
    "    print('F1 = ' + str(round(f1, 3)))\n",
    "          \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes (Baseline Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = BernoulliNB()\n",
    "accuracy_nb, precision_nb, recall_nb, f1_nb = kfold_cv(10, X, y, nb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "accuracy_rf, precision_rf, recall_rf, f1_rf = kfold_cv(10, X, y, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "accuracy_gbm, precision_gbm, recall_gbm, f1_gbm = kfold_cv(10, X, y, lgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "accuracy_xgb, precision_xgb, recall_xgb, f1_xgb = kfold_cv(10, X, y, xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the accuracy and the recall comparisons from the different algorithms, we decide to choose Random Forest as our most optimal algorithms because it has the highest recall among the four algorithms. In the next step, we will continue the process in searching for the hyperparameters that optimize our model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameters Tuning through Grid Search - Flame\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Model Evaluation - Yejia\n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before? ROC curve, confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = pd.DataFrame({'Model': ['Naive Bayes', 'Random Forest', 'LightGBM', 'XGBoost'], \n",
    "#                     'Accuracy': [**to be filled in**]})                                                                          \n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall = pd.DataFrame({'Model': ['Naive Bayes', 'Random Forest', 'LightGBM', 'XGBoost'], \n",
    "#                     'Recall': [**to be filled in**]})                                                                          \n",
    "# recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_nb = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_nb))\n",
    "# cm_nb.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_rf = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_rf))\n",
    "# cm_rf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_gbm = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_gbm))\n",
    "# cm_gbm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_xgb = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_xgb))\n",
    "# cm_xgb.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result - Li ge\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations - Li ge\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "First of all, since this is a public dataset found on Kaggle, it’s likely that it is rather safe regarding ethics and data privacy concerns. Upon examining the dataset, although we have transaction id, there’s no way we could trace it back to find personally identifiable information. Moreover, although we have payment card information, we don’t have card number, but only card type, card category, issue bank, and country, which also isn’t enough to extract personally identifiable information. For the identity part of this dataset, it’s unlikely that there could be ethical or privacy concerns because we don’t actually know what the fields stand for, as they are simply called id2, id3, etc. According to the descriptions, The field names are masked and pairwise dictionaries will not be provided for privacy protection and contract agreement. However, this dataset could be somewhat biased because according to its description, the data is collected by Vesta’s fraud protection system and digital security partners, which is only one or several companies. Thus, this dataset is not guaranteed to be an accurate representation of all transactions that are fraud or not. It is also unlikely that our results could be used unethically because our task tries to identify and prevent frauds, without violating data privacy and without proposing individuals to engage in these illegal activities. \n",
    "\n",
    "### Conclusion - Yejia\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Sasso\"></a>1.[^](#Sasso): Sasso, T. (14 Feb 2023) What is Online Transaction Fraud? *Funraise.* https://help.funraise.io/en/articles/3944864-what-is-online-transaction-fraud#:~:text=What%20is%20Transaction%20Fraud%3F%20Transaction%20fraud%20is%20a,a%20transaction%20in%20their%20name%E2%80%94like%20stolen%20credit%20cards. <br> \n",
    "<a name=\"Skiba\"></a>2.[^](#Skiba): Skiba, K. (22 Feb 2022) Consumer Fraud Losses Hit Record $5.8 Billion. *AARP.* https://www.aarp.org/money/scams-fraud/info-2022/ftc-fraud-report-new.html <br>\n",
    "<a name=\"McKee\"></a>3.[^](#McKee): McKee, J. (22 Nov 2022) Unpacking The Overall Impact of Fraud. *Forbes.* https://www.forbes.com/sites/jordanmckee/2020/11/22/unpacking-the-overall-impact-of-fraud/?sh=560a75917891 <br>\n",
    "<a name=\"Oakley\"></a>4.[^](#Oakley): Oakley, C. (30 March 2022) The role of machine learning in fraud detection. *Featurespace.* https://www.featurespace.com/newsroom/the-role-of-machine-learning-in-fraud-detection/<br> \n",
    "<a name=\"Pykes\"></a>5.[^](#Pykes): Pykes, K. (21 Sep 2020) Using Machine Learning To Detect Fraud. *Towards Data Science.* https://towardsdatascience.com/using-machine-learning-to-detect-fraud-f204910389cf <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
